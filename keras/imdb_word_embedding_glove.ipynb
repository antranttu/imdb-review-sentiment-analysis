{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'plaidml.keras.backend'\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\antra\\\\OneDrive\\\\kaggle\\\\sentiment_analysis\\\\keras'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '.Rhistory',\n",
       " 'imdb_binary_10000_words.csv',\n",
       " 'imdb_binary_20000_words.csv',\n",
       " 'imdb_binary_30000_words.csv',\n",
       " 'imdb_binary_representation.ipynb',\n",
       " 'imdb_keras.Rmd',\n",
       " 'imdb_keras2.Rmd',\n",
       " 'imdb_word_embedding_128batch.csv',\n",
       " 'imdb_word_embedding_128batch_200words.csv',\n",
       " 'imdb_word_embedding_128batch_500words.csv',\n",
       " 'imdb_word_embedding_128batch_own_embedding.csv',\n",
       " 'imdb_word_embedding_128batch_own_embedding_300d.csv',\n",
       " 'imdb_word_embedding_32batch.csv',\n",
       " 'imdb_word_embedding_512batch.csv',\n",
       " 'imdb_word_embedding_glove.ipynb',\n",
       " 'labeledTrainData.tsv',\n",
       " 'NN_processed_words_tf.csv',\n",
       " 'NN_processed_words_tfidf.csv',\n",
       " 'NN_processed_words_tfidf_10739words.csv',\n",
       " 'NN_processed_words_tfidf_10739words_binarized.csv',\n",
       " 'NN_processed_words_tfidf_4714words.csv',\n",
       " 'NN_processed_words_tfidf_4714words_binarized.csv',\n",
       " 'NN_processed_words_tfidf_6023words.csv',\n",
       " 'NN_processed_words_tfidf_7164words.csv',\n",
       " 'NN_processed_words_tf_4716words_binarized.csv',\n",
       " 'NN_processed_words_tf_6023words.csv',\n",
       " 'NN_processed_words_tf_6120words_binarized.csv',\n",
       " 'NN_processed_words_tf_9792words_binarized.csv',\n",
       " 'NN_unprocessed_words_tf.csv',\n",
       " 'NN_unprocessed_words_tfidf.csv',\n",
       " 'NN_unprocessed_words_tfidf_15343words.csv',\n",
       " 'NN_unprocessed_words_tfidf_6120words.csv',\n",
       " 'NN_unprocessed_words_tfidf_6120words_binarized.csv',\n",
       " 'NN_unprocessed_words_tfidf_9792words.csv',\n",
       " 'NN_unprocessed_words_tf_6000words.csv',\n",
       " 'NN_unprocessed_words_tf_9792words.csv',\n",
       " 'own_embedding_128batch_100d.h5',\n",
       " 'own_embedding_128batch_300d.h5',\n",
       " 'pre_trained_glove.h5',\n",
       " 'testData.tsv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file using relative path\n",
    "train = pd.read_csv('labeledTrainData.tsv', sep='\\t')\n",
    "test = pd.read_csv('testData.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>3453_3</td>\n",
       "      <td>0</td>\n",
       "      <td>It seems like more consideration has gone into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>5064_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't believe they made this film. Completel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>10905_3</td>\n",
       "      <td>0</td>\n",
       "      <td>Guy is a loser. Can't get girls, needs to buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>10194_3</td>\n",
       "      <td>0</td>\n",
       "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>8478_8</td>\n",
       "      <td>1</td>\n",
       "      <td>I saw this movie as a child and it broke my he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sentiment                                             review\n",
       "0       5814_8          1  With all this stuff going down at the moment w...\n",
       "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3       3630_4          0  It must be assumed that those who praised this...\n",
       "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
       "...        ...        ...                                                ...\n",
       "24995   3453_3          0  It seems like more consideration has gone into...\n",
       "24996   5064_1          0  I don't believe they made this film. Completel...\n",
       "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
       "24998  10194_3          0  This 30 minute documentary Buñuel made in the ...\n",
       "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some **cleaning** in our texts: lower case; remove punctuations, remove numbers, replace abbreviations, etc. Because keras **tokenizer** does the punctuations removal; we don't have to manually do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all words in text\n",
    "def to_lower(docs):\n",
    "    docs = [doc.lower() for doc in docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for non-ascii character\n",
    "def is_alpha(char):\n",
    "    return char in string.ascii_lowercase\n",
    "\n",
    "# only keep numbers and ascii characters. replace non-ascii characters with space\n",
    "def keep_alphanumeric(doc):\n",
    "    doc = [char for char in doc]\n",
    "    out = ''\n",
    "    for char in doc:\n",
    "        good = is_alpha(char) or char.isnumeric() # or char in [' ', '.']\n",
    "        if good:\n",
    "            out += char\n",
    "        else:\n",
    "            out += ' '\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "def remove_special(docs):\n",
    "    docs = [keep_alphanumeric(doc) for doc in docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers\n",
    "def remove_numbers(docs):\n",
    "    docs = [re.sub('\\d+', ' ', doc) for doc in docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes extra whitespace for a list of text strings\n",
    "def remove_whitespace(docs):\n",
    "    docs = [' '.join(doc.split()) for doc in docs]\n",
    "    docs = [doc.rstrip() for doc in docs]\n",
    "    docs = [doc.lstrip() for doc in docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review'] = to_lower(train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review'] = remove_special(train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review'] = remove_numbers(train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review'] = remove_whitespace(train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 73276 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "max_words = 10000 # considers only the top 10000 words in the dataset\n",
    "maxlen = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words) # creates a tokenizer, takes only the first 10000 common words\n",
    "tokenizer.fit_on_texts(train['review']) # build the word index\n",
    "sequences = tokenizer.texts_to_sequences(train['review']) # turns strings into lists of integer indices\n",
    "\n",
    "word_index = tokenizer.word_index # how to recover the word index that was computed\n",
    "print('Found %s unique tokens.' % len(word_index)) # how many tokens are there in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decode the indices of one of the **sequences** back into original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with all this stuff going down at the moment with mj i ve started listening to his music watching the odd documentary here and there watched the and watched again maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent is part biography part feature film which i remember going to see at the cinema when it was originally released some of it has subtle messages about mj s feeling towards the press and also the obvious message of drugs are bad m kay br br visually impressive but of course this is all about michael jackson so unless you remotely like mj in anyway then you are going to hate this and find it boring some may call mj an for to the making of this movie but mj and most of his fans would say that he made it for the fans which if true is really nice of him br br the actual feature film bit when it finally starts is only on for minutes or so the smooth criminal sequence and joe is convincing as a all powerful drug lord why he wants mj dead so bad is beyond me because mj his plans joe s character that he wanted people to know it is he who is drugs etc so i maybe he just hates mj s music br br lots of cool things in this like mj turning into a car and a robot and the whole speed demon sequence also the director must have had the patience of a saint when it came to filming the bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene br br bottom line this movie is for people who like mj on one level or another which i think is most people if not then stay away it does try and give off a wholesome message and ironically mj s buddy in this movie is a girl michael jackson is truly one of the most talented people ever to grace this planet but is he guilty well with all the attention i ve gave this subject hmmm well i don t know because people can be different behind closed doors i know this for a fact he is either an extremely nice but stupid guy or one of the most i hope he is not the latter'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i, '?') for i in sequences[0]])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pad our word indices with 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen) # truncate texts after \"maxlen\", or fill in with 0's if not long enough\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,   17,   31,   11,  529,\n",
       "        169,  180,   32,    1,  546,   17, 8964,   10,  138,  635, 2600,\n",
       "          5,   27,  223,  148,    1, 1009,  644,  127,    2,   40,  293,\n",
       "          1,    2,  293,  173,  278,   10,   43,  181,    5,   75,    3,\n",
       "        794, 2601,   83,   11,  225,   36,   10,  195,   14,   65,  627,\n",
       "          9,    1, 4220,   43,    5,  278,   95,   56,   60,  327,  711,\n",
       "         25,    6, 2476,   42, 1331,    6,  172, 4972,  172,  768,   20,\n",
       "         62,   10,  371,  169,    5,   66,   32,    1,  423,   54,    8,\n",
       "         14, 1803,  615,   49,    4,    8,   47, 1279, 3409,   44, 8964,\n",
       "         13,  537,  929,    1, 3486,    2,   82,    1,  567,  730,    4,\n",
       "       1640,   26,   76,  141, 4569,    7,    7, 1993, 1134,   19,    4,\n",
       "        259,   11,    6,   31,   44,  466, 1580,   37,  878,   22, 2565,\n",
       "         39, 8964,    9,  544,   93,   22,   26,  169,    5,  770,   11,\n",
       "          2,  168,    8,  353,   49,  199,  669, 8964,   35,   16,    5,\n",
       "          1,  228,    4,   11,   18,   19, 8964,    2,   90,    4,   27,\n",
       "        446,   61,  133,   12,   25,   91,    8,   16,    1,  446,   62,\n",
       "         46,  280,    6,   65,  324,    4,   88,    7,    7,    1,  766,\n",
       "        768,   20,  224,   54,    8,  413,  506,    6,   64,   23,   16,\n",
       "        229,   42,   37,    1, 3460, 1635,  704,    2,  867,    6, 1060,\n",
       "         15,    3,   31,  957, 1376, 1547,  135,   25,  484, 8964,  338,\n",
       "         37,   76,    6,  706,   71,   86, 8964,   27, 2415,  867,   13,\n",
       "        104,   12,   25,  465,   78,    5,  122,    8,    6,   25,   36,\n",
       "          6, 1640,  512,   37,   10,  278,   25,   43, 4107, 8964,   13,\n",
       "        223,    7,    7,  761,    4,  627,  182,    9,   11,   39, 8964,\n",
       "       1568,   83,    3,  504,    2,    3, 2304,    2,    1,  222, 2061,\n",
       "       2644,  704,   82,    1,  157,  207,   28,   68,    1, 5021,    4,\n",
       "          3, 5245,   54,    8,  379,    5, 1405,    1,   76,  704,   15,\n",
       "        621,  879,  770,  765,   17,   30,  518,  277,  572,    3,  222,\n",
       "        746,    4,   96, 3410,    3, 1297,  816,  134,    7,    7, 1308,\n",
       "        341,   11,   18,    6,   16,   78,   36,   39, 8964,   23,   30,\n",
       "        636,   42,  160,   62,   10,  102,    6,   90,   78,   46,   24,\n",
       "         93,  774,  243,    8,  126,  348,    2,  201,  123,    3, 7574,\n",
       "        730,    2, 3583, 8964,   13, 2005,    9,   11,   18,    6,    3,\n",
       "        237,  466, 1580,    6,  365,   30,    4,    1,   90, 1000,   78,\n",
       "        124,    5, 1627,   11, 1133,   19,    6,   25, 2476,   72,   17,\n",
       "         31,    1,  677,   10,  138,  510,   11,  851, 7069,   72,   10,\n",
       "         89,   21,  122,   86,   78,   51,   29,  271,  486, 4539, 3531,\n",
       "         10,  122,   11,   16,    3,  190,   25,    6,  343,   35,  564,\n",
       "        324,   19,  372,  225,   42,   30,    4,    1,   90,   10,  439,\n",
       "         25,    6,   24,    1, 1495])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the GloVe word embeddings with 100 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_dir = 'C:\\\\Users\\\\antra\\\\OneDrive\\\\kaggle\\\\sentiment_analysis\\\\word_embedding\\\\glove.6B'\n",
    "\n",
    "# embeddings_index = {}\n",
    "# f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), errors='ignore', encoding='utf8')\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, build an embedding matrix that can load into Embedding layer. It must be a matrix of shape (max_words, embedding_dim). **Note**: index 0 isn't supposed to stand for any word or token; it's simply a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 500\n",
    "\n",
    "# embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "# for word, i in word_index.items():\n",
    "#     if i < max_words:\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[i] = embedding_vector # words not found in embedding index will be all 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"opencl_amd_ellesmere.0\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 500)          5000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 250000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                4000016   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 9,000,305\n",
      "Trainable params: 9,000,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen)) # vocabulary size; embedding dimensions; length of each document\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Embedding** layer has a single weight matrix: a 2D float matrix where each entry is the word vector associated with index i. Let's load the GloVe matrix that we prepared into the Embedding layer, the first layer in the model. We will freeze the Embedding layer (*trainable* to *False*) because pre-trained parts shouldn't be updated during training.\n",
    "\n",
    "If we do not want to use GloVe pre-trained word embeddings; we can disable the pre-trained weights and let the model learn the embedding weights on their own for this task specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].set_weights([embedding_matrix])\n",
    "# model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 500)          5000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 250000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                4000016   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 9,000,305\n",
      "Trainable params: 9,000,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate how well our model performs on unseen data; we need to have a validation set. Let's use **20%** of our **training data** as **validation data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = data[:5000]\n",
    "partial_x_train = data[5000:]\n",
    "\n",
    "y_val = np.asarray(train['sentiment'][:5000]) # convert labels/outputs to arrays because it's faster\n",
    "partial_y_train = np.asarray(train['sentiment'][5000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "20000/20000 [==============================] - 1565s 78ms/step - loss: 0.6395 - acc: 0.6180 - val_loss: 0.3517 - val_acc: 0.8522\n",
      "Epoch 2/7\n",
      "20000/20000 [==============================] - 1723s 86ms/step - loss: 0.2270 - acc: 0.9093 - val_loss: 0.2965 - val_acc: 0.8778\n",
      "Epoch 3/7\n",
      "20000/20000 [==============================] - 1740s 87ms/step - loss: 0.0451 - acc: 0.9865 - val_loss: 0.3867 - val_acc: 0.8676\n",
      "Epoch 4/7\n",
      "20000/20000 [==============================] - 1736s 87ms/step - loss: 0.0079 - acc: 0.9978 - val_loss: 0.5055 - val_acc: 0.8750\n",
      "Epoch 5/7\n",
      "20000/20000 [==============================] - 1727s 86ms/step - loss: 0.0041 - acc: 0.9985 - val_loss: 0.5790 - val_acc: 0.8654\n",
      "Epoch 6/7\n",
      "20000/20000 [==============================] - 1855s 93ms/step - loss: 6.1980e-04 - acc: 0.9997 - val_loss: 0.6638 - val_acc: 0.8650\n",
      "Epoch 7/7\n",
      "20000/20000 [==============================] - 1762s 88ms/step - loss: 0.0076 - acc: 0.9979 - val_loss: 0.6813 - val_acc: 0.8676\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=7,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_val, y_val))\n",
    "# model.save_weights('pre_trained_glove.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the model's performance overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model starts overfitting after around 7 epochs. Let's re-train the model again for only 7 epochs using all data. First, let's redefine the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 500)          5000000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 250000)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                4000016   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 9,000,305\n",
      "Trainable params: 9,000,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen)) # vocabulary size; embedding dimensions; length of each document\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we do not want to use GloVe pre-trained word embeddings; let's disable the pre-trained weights and let the model learn the embedding weights on their own for this task specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].set_weights([embedding_matrix])\n",
    "# model.layers[0].trainable = False\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 1778s 71ms/step - loss: 0.6374 - acc: 0.6044\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 1778s 71ms/step - loss: 0.2373 - acc: 0.9027\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 1829s 73ms/step - loss: 0.0512 - acc: 0.9832\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 1778s 71ms/step - loss: 0.0091 - acc: 0.9970\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 17298s 692ms/step - loss: 0.0029 - acc: 0.9989\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(data,\n",
    "                    np.asarray(train['sentiment']),\n",
    "                    epochs=5,\n",
    "                    batch_size=128)\n",
    "# model.save_weights('pre_trained_glove.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our finalized training model; let's use it to **predict** the **test set**. First, we are going to **clean up** the **test set** exactly like we did with the **training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('testData.tsv', sep='\\t')\n",
    "\n",
    "test['review'] = to_lower(test['review'])\n",
    "test['review'] = remove_special(test['review'])\n",
    "test['review'] = remove_numbers(test['review'])\n",
    "test['review'] = remove_whitespace(test['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the texts in our test set cleaned up; let's **tokenize** the texts based on the **Tokenizer on training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(test['review'])\n",
    "test_data = pad_sequences(sequences_test, maxlen=maxlen) # truncate texts after \"maxlen\", or fill in with 0's if not long enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('pre_trained_glove.h5') # load the pre-trained weights before\n",
    "\n",
    "predictions = model.predict(test_data) # these are probabilities that it belongs to positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = np.where(predictions >= 0.5, 1, np.where(predictions < 0.5, 0, predictions)) # if probabilities > 0.5; sentiment = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('imdb_word_embedding_128batch_own_embedding_500d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('own_embedding_128batch_500d.h5') # save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
